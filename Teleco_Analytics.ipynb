{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('script')\n",
    "from script import dbconn\n",
    "pgconn = dbconn.db_connection_psycopg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fectching data from the postgreSql database and put the value on raw_df\n",
    "raw_df = dbconn.db_read_table_psycopg(pgconn,'xdr_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Overview analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the top 10 handsets used by the customers.\n",
    "# Count handset usage\n",
    "handset_counts = raw_df.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='Count')\n",
    "\n",
    "# Sort the results\n",
    "sorted_handsets = handset_counts.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Select the top 10 handsets\n",
    "top_10_handsets = sorted_handsets.head(10)\n",
    "\n",
    "# Print the top 10 handsets\n",
    "print(top_10_handsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying Top 3 handset manufacturers\n",
    "manufacturer_counts = raw_df['Handset Manufacturer'].value_counts().reset_index()\n",
    "manufacturer_counts.columns = ['Handset Manufacturer', 'Count']\n",
    "\n",
    "# Sort the results\n",
    "sorted_manufacturers = manufacturer_counts.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Select the top 3 handset manufacturers\n",
    "top_3_manufacturers = sorted_manufacturers.head(3)\n",
    "\n",
    "# Print the top 3 handset manufacturers\n",
    "print(top_3_manufacturers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the top 5 handsets per top 3 handset manufacturer\n",
    "# Count handset manufacturers and types\n",
    "manufacturer_type_counts = raw_df.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='Count')\n",
    "\n",
    "# Sort the results within each manufacturer\n",
    "sorted_manufacturer_types = manufacturer_type_counts.groupby('Handset Manufacturer').apply(lambda x: x.nlargest(5, 'Count')).reset_index(drop=True)\n",
    "\n",
    "# Print the top 5 handsets per top 3 handset manufacturers\n",
    "top_3_manufacturers = sorted_manufacturer_types['Handset Manufacturer'].unique()[:3]\n",
    "\n",
    "for manufacturer in top_3_manufacturers:\n",
    "    print(f\"Top 5 handsets for {manufacturer}:\")\n",
    "    manufacturer_handsets = sorted_manufacturer_types[sorted_manufacturer_types['Handset Manufacturer'] == manufacturer]\n",
    "    print(manufacturer_handsets)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying users’ behaviour on those Applications / Social Media, Google, Email, Youtube, Netflix, Gaming, Other.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the number of xDR sessions per user\n",
    "sessions_per_user = raw_df.groupby('MSISDN/Number')['Bearer Id'].count()\n",
    "print(sessions_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the session duration per user\n",
    "session_duration_per_user = raw_df.groupby('MSISDN/Number')['Dur. (ms)'].sum()\n",
    "\n",
    "# Display the result\n",
    "print(session_duration_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the total download (Dl) and upload data per user\n",
    "total_data_per_user = raw_df.groupby('MSISDN/Number')[['Total DL (Bytes)', 'Total UL (Bytes)']].sum()\n",
    "\n",
    "# Display the result\n",
    "print(total_data_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of application columns\n",
    "applications = ['Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']\n",
    "\n",
    "# Aggregate the total data volume per user and application\n",
    "total_data_per_user_app = raw_df.groupby('MSISDN/Number')[[col + ' DL (Bytes)' for col in applications] + [col + ' UL (Bytes)' for col in applications]].sum()\n",
    "\n",
    "# Display the result\n",
    "print(total_data_per_user_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanatory Data Analysis | EDA\n",
    "# Treat all missing values and outliers in the dataset by replacing by the mean of the corresponding column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Identifying and Treating missing value \n",
    "missing_values = raw_df.isna().sum()\n",
    "print (missing_values)\n",
    "\n",
    "for column in missing_values.index:\n",
    "    if missing_values[column] > 0:\n",
    "        mean_value = str(raw_df[column].mean())\n",
    "        raw_df[column] = raw_df[column].fillna(mean_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the basic metrics (mean, median, etc) from the dataset\n",
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct a Non-Graphical Univariate Analysis by computing dispersion parameters for each quantitative variable.\n",
    "\n",
    "quantitative_variables = []\n",
    "\n",
    "# Iterate over each column in the dataset to find the quantitative variable\n",
    "for column in raw_df.columns:   \n",
    "    if raw_df[column].dtype in [int, float]:\n",
    "        quantitative_variables.append(column)\n",
    "\n",
    "# Solution: By calculating the range or Difference b/n max and min value in each variable\n",
    "for column_name in quantitative_variables:\n",
    "    column_data = raw_df[column_name]\n",
    "    data_range = column_data.max() - column_data.min()\n",
    "    print(\"Range of\", column_name, \":\", data_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct a Graphical Univariate Analysis by identifying the most suitable plotting options for each variable and interpret your findings.\n",
    "# column_name = 'Avg RTT DL (ms)'\n",
    "clean_Data = raw_df.dropna()\n",
    "column_names = clean_Data.columns\n",
    "\n",
    "for column_name in column_names:\n",
    "    column_data = clean_Data[column_name]    \n",
    "    plt.hist(column_data, bins=10)\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of ' + column_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable transformations\n",
    "# Segment the users into top five decile classes based on the total duration for all sessions and compute the total data (DL+UL) per decile class. \n",
    "\n",
    "# Calculate the total duration for all sessions for each user\n",
    "user_total_duration = raw_df.groupby('MSISDN/Number')['Dur. (ms)'].sum()\n",
    "\n",
    "# Segment users into decile classes\n",
    "user_deciles = pd.qcut(user_total_duration, q=10, labels=False, duplicates='drop')\n",
    "\n",
    "# Compute the total data (DL+UL) per decile class\n",
    "data_per_decile = raw_df.groupby(user_deciles)[['Total DL (Bytes)', 'Total UL (Bytes)']].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis – compute a correlation matrix for the following variables and interpret your findings: Social Media data, Google data, Email data, Youtube data, Netflix data, Gaming data, Other data \n",
    "# Select the columns for correlation analysis\n",
    "columns = [\n",
    "    'Social Media DL (Bytes)', 'Social Media UL (Bytes)',\n",
    "    'Google DL (Bytes)', 'Google UL (Bytes)',\n",
    "    'Email DL (Bytes)', 'Email UL (Bytes)',\n",
    "    'Youtube DL (Bytes)', 'Youtube UL (Bytes)',\n",
    "    'Netflix DL (Bytes)', 'Netflix UL (Bytes)',\n",
    "    'Gaming DL (Bytes)', 'Gaming UL (Bytes)',\n",
    "    'Other DL (Bytes)', 'Other UL (Bytes)'\n",
    "]\n",
    "\n",
    "# Create a subset dataframe with the selected columns\n",
    "subset_df = raw_df[columns]\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = subset_df.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  User Engagement analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the above metrics per customer id (MSISDN) and report the top 10 customers per engagement metric\n",
    "\n",
    "# Aggregate metrics per customer\n",
    "aggregated_data = raw_df.groupby('MSISDN/Number').agg({\n",
    "    'Bearer Id': 'nunique',  # Session frequency\n",
    "    'Dur. (ms)': 'sum',  # Session duration\n",
    "    'Total UL (Bytes)': 'sum',  # Upload traffic\n",
    "    'Total DL (Bytes)': 'sum'  # Download traffic\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns for better readability\n",
    "aggregated_data.rename(columns={\n",
    "    'Bearer Id': 'Session Frequency',\n",
    "    'Dur. (ms)': 'Session Duration',\n",
    "    'Total UL (Bytes)': 'Total Upload Traffic',\n",
    "    'Total DL (Bytes)': 'Total Download Traffic'\n",
    "}, inplace=True)\n",
    "\n",
    "# Report the top 10 customers per engagement metric\n",
    "top_10_frequency = aggregated_data.nlargest(10, 'Session Frequency')\n",
    "top_10_duration = aggregated_data.nlargest(10, 'Session Duration')\n",
    "top_10_upload_traffic = aggregated_data.nlargest(10, 'Total Upload Traffic')\n",
    "top_10_download_traffic = aggregated_data.nlargest(10, 'Total Download Traffic')\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 customers by Session Frequency:\")\n",
    "print(top_10_frequency)\n",
    "\n",
    "print(\"\\nTop 10 customers by Session Duration:\")\n",
    "print(top_10_duration)\n",
    "\n",
    "print(\"\\nTop 10 customers by Total Upload Traffic:\")\n",
    "print(top_10_upload_traffic)\n",
    "\n",
    "print(\"\\nTop 10 customers by Total Download Traffic:\")\n",
    "print(top_10_download_traffic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each engagement metric and run a k-means (k=3) to classify customers in three groups of engagement. \n",
    "# Aggregate metrics per customer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "aggregated_data = raw_df.groupby('MSISDN/Number').agg({\n",
    "    'Bearer Id': 'nunique',  # Session frequency\n",
    "    'Dur. (ms)': 'sum',  # Session duration\n",
    "    'Total UL (Bytes)': 'sum',  # Upload traffic\n",
    "    'Total DL (Bytes)': 'sum'  # Download traffic\n",
    "}).reset_index()\n",
    "\n",
    "# Normalize the engagement metrics\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(aggregated_data.iloc[:, 1:])  # Exclude customer ID\n",
    "\n",
    "# Run k-means clustering\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(normalized_data)\n",
    "\n",
    "# Add the cluster labels to the aggregated data\n",
    "aggregated_data['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Report the top customers per cluster\n",
    "top_customers_per_cluster = []\n",
    "for i in range(k):\n",
    "    cluster_customers = aggregated_data[aggregated_data['Cluster'] == i].nlargest(10, 'Bearer Id')\n",
    "    top_customers_per_cluster.append(cluster_customers)\n",
    "\n",
    "# Display the results\n",
    "for i, cluster_customers in enumerate(top_customers_per_cluster):\n",
    "    print(f\"\\nTop 10 customers in Cluster {i+1}:\")\n",
    "    print(cluster_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum, maximum, average & total non-normalized metrics for each cluster. \n",
    "# Interpret your results visually with accompanying text explaining your findings.\n",
    "\n",
    "# Aggregate metrics per customer\n",
    "aggregated_data = raw_df.groupby('MSISDN/Number').agg({\n",
    "    'Bearer Id': 'nunique',  # Session frequency\n",
    "    'Dur. (ms)': 'sum',  # Session duration\n",
    "    'Total UL (Bytes)': 'sum',  # Upload traffic\n",
    "    'Total DL (Bytes)': 'sum'  # Download traffic\n",
    "}).reset_index()\n",
    "\n",
    "# Normalize the engagement metrics\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(aggregated_data.iloc[:, 1:])  # Exclude customer ID\n",
    "\n",
    "# Run k-means clustering\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(normalized_data)\n",
    "\n",
    "# Add the cluster labels to the aggregated data\n",
    "aggregated_data['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Compute non-normalized metrics for each cluster\n",
    "cluster_metrics = aggregated_data.groupby('Cluster').agg({\n",
    "    'Bearer Id': ['min', 'max', 'mean', 'sum'],  # Session frequency\n",
    "    'Dur. (ms)': ['min', 'max', 'mean', 'sum'],  # Session duration\n",
    "    'Total UL (Bytes)': ['min', 'max', 'mean', 'sum'],  # Upload traffic\n",
    "    'Total DL (Bytes)': ['min', 'max', 'mean', 'sum']  # Download traffic\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "print(\"Non-normalized metrics for each cluster:\")\n",
    "print(cluster_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate user total traffic per application and derive the top 10 most engaged users per application\n",
    "\n",
    "# Extract the relevant columns for application traffic\n",
    "app_columns = ['MSISDN/Number', 'Social Media DL (Bytes)', 'Google DL (Bytes)', \n",
    "               'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', \n",
    "               'Gaming DL (Bytes)', 'Other DL (Bytes)']\n",
    "app_traffic = raw_df[app_columns].copy()\n",
    "\n",
    "# Rename the application columns for easier processing\n",
    "app_traffic.columns = ['MSISDN/Number', 'Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']\n",
    "\n",
    "# Melt the dataframe to combine all application columns into a single 'Application' column\n",
    "app_traffic = app_traffic.melt(id_vars='MSISDN/Number', var_name='Application', value_name='Total Traffic')\n",
    "\n",
    "# Aggregate total traffic per application for each user\n",
    "app_traffic = app_traffic.groupby(['MSISDN/Number', 'Application'])['Total Traffic'].sum().reset_index()\n",
    "\n",
    "# Derive the top 10 most engaged users per application\n",
    "top_users_per_app = []\n",
    "unique_apps = app_traffic['Application'].unique()\n",
    "\n",
    "for app in unique_apps:\n",
    "    top_users = app_traffic[app_traffic['Application'] == app].nlargest(10, 'Total Traffic')\n",
    "    top_users_per_app.append(top_users)\n",
    "\n",
    "# Display the results\n",
    "for i, app in enumerate(unique_apps):\n",
    "    print(f\"\\nTop 10 most engaged users for Application '{app}':\")\n",
    "    print(top_users_per_app[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 3 most used applications using appropriate charts. \n",
    "\n",
    "# Extract the relevant columns for application traffic\n",
    "app_columns = ['MSISDN/Number', 'Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']\n",
    "app_traffic = raw_df[app_columns].copy()\n",
    "\n",
    "# Rename the application columns for easier processing\n",
    "app_traffic.columns = ['MSISDN/Number', 'Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']\n",
    "\n",
    "# Melt the dataframe to combine all application columns into a single 'Application' column\n",
    "app_traffic = app_traffic.melt(id_vars='MSISDN/Number', var_name='Application', value_name='Total Traffic')\n",
    "\n",
    "# Aggregate total traffic per application\n",
    "app_traffic = app_traffic.groupby('Application')['Total Traffic'].sum().reset_index()\n",
    "\n",
    "# Sort applications by total traffic in descending order\n",
    "app_traffic = app_traffic.sort_values('Total Traffic', ascending=False)\n",
    "\n",
    "# Select the top 3 most used applications\n",
    "top_3_apps = app_traffic.head(3)\n",
    "\n",
    "# Plot the top 3 most used applications\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(top_3_apps['Application'], top_3_apps['Total Traffic'])\n",
    "plt.xlabel('Application')\n",
    "plt.ylabel('Total Traffic')\n",
    "plt.title('Top 3 Most Used Applications')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using k-means clustering algorithm, group users in k engagement clusters based on the engagement metrics: \n",
    "# What is the optimized value of k (use elbow method for this)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
