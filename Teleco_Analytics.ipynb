{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('script')\n",
    "from script import dbconn\n",
    "pgconn = dbconn.db_connection_psycopg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fectching data from the postgreSql database and put the value on raw_df\n",
    "raw_df = dbconn.db_read_table_psycopg(pgconn,'xdr_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Overview analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the top 10 handsets used by the customers.\n",
    "# Count handset usage\n",
    "handset_counts = raw_df.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='Count')\n",
    "\n",
    "# Sort the results\n",
    "sorted_handsets = handset_counts.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Select the top 10 handsets\n",
    "top_10_handsets = sorted_handsets.head(10)\n",
    "\n",
    "# Print the top 10 handsets\n",
    "print(top_10_handsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying Top 3 handset manufacturers\n",
    "manufacturer_counts = raw_df['Handset Manufacturer'].value_counts().reset_index()\n",
    "manufacturer_counts.columns = ['Handset Manufacturer', 'Count']\n",
    "\n",
    "# Sort the results\n",
    "sorted_manufacturers = manufacturer_counts.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Select the top 3 handset manufacturers\n",
    "top_3_manufacturers = sorted_manufacturers.head(3)\n",
    "\n",
    "# Print the top 3 handset manufacturers\n",
    "print(top_3_manufacturers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the top 5 handsets per top 3 handset manufacturer\n",
    "# Count handset manufacturers and types\n",
    "manufacturer_type_counts = raw_df.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='Count')\n",
    "\n",
    "# Sort the results within each manufacturer\n",
    "sorted_manufacturer_types = manufacturer_type_counts.groupby('Handset Manufacturer').apply(lambda x: x.nlargest(5, 'Count')).reset_index(drop=True)\n",
    "\n",
    "# Print the top 5 handsets per top 3 handset manufacturers\n",
    "top_3_manufacturers = sorted_manufacturer_types['Handset Manufacturer'].unique()[:3]\n",
    "\n",
    "for manufacturer in top_3_manufacturers:\n",
    "    print(f\"Top 5 handsets for {manufacturer}:\")\n",
    "    manufacturer_handsets = sorted_manufacturer_types[sorted_manufacturer_types['Handset Manufacturer'] == manufacturer]\n",
    "    print(manufacturer_handsets)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying users’ behaviour on those Applications / Social Media, Google, Email, Youtube, Netflix, Gaming, Other.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the number of xDR sessions per user\n",
    "sessions_per_user = raw_df.groupby('MSISDN/Number')['Bearer Id'].count()\n",
    "print(sessions_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the session duration per user\n",
    "session_duration_per_user = raw_df.groupby('MSISDN/Number')['Dur. (ms)'].sum()\n",
    "\n",
    "# Display the result\n",
    "print(session_duration_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the total download (Dl) and upload data per user\n",
    "total_data_per_user = raw_df.groupby('MSISDN/Number')[['Total DL (Bytes)', 'Total UL (Bytes)']].sum()\n",
    "\n",
    "# Display the result\n",
    "print(total_data_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of application columns\n",
    "applications = ['Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']\n",
    "\n",
    "# Aggregate the total data volume per user and application\n",
    "total_data_per_user_app = raw_df.groupby('MSISDN/Number')[[col + ' DL (Bytes)' for col in applications] + [col + ' UL (Bytes)' for col in applications]].sum()\n",
    "\n",
    "# Display the result\n",
    "print(total_data_per_user_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanatory Data Analysis | EDA\n",
    "# Treat all missing values and outliers in the dataset by replacing by the mean of the corresponding column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percent of missing data\n",
    "def percent_missing(df):\n",
    "    # Calculate total number of cells in dataframe\n",
    "    totalCells = np.product(df.shape)\n",
    "\n",
    "    # Count number of missing values per column\n",
    "    missingCount = df.isnull().sum()\n",
    "\n",
    "    # Calculate total number of missing values\n",
    "    totalMissing = missingCount.sum()\n",
    "\n",
    "    # Calculate percentage of missing values\n",
    "    percentageMissing = (totalMissing / totalCells) * 100\n",
    "\n",
    "    print(\"The dataset contains\", round(percentageMissing, 2), \"%\", \"missing values.\")\n",
    "\n",
    "percent_missing(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with column mean for numeric columns\n",
    "numeric_columns = raw_df.select_dtypes(include=[np.number]).columns\n",
    "raw_df[numeric_columns] = raw_df[numeric_columns].fillna(raw_df[numeric_columns].mean())\n",
    "\n",
    "# Replace missing values with column mode for non-numeric columns\n",
    "non_numeric_columns = raw_df.select_dtypes(exclude=[np.number]).columns\n",
    "raw_df[non_numeric_columns] = raw_df[non_numeric_columns].fillna(raw_df[non_numeric_columns].mode().iloc[0])\n",
    "\n",
    "# Identify and replace outliers with column mean for numeric columns\n",
    "for col in numeric_columns:\n",
    "    z_scores = (raw_df[col] - raw_df[col].mean()) / raw_df[col].std()\n",
    "    outliers = (z_scores > 3) | (z_scores < -3)\n",
    "    raw_df.loc[outliers, col] = raw_df[col].mean()\n",
    "\n",
    "# Verify missing values and outliers have been treated\n",
    "missing_values_after_treatment = raw_df.isnull().sum()\n",
    "print(\"Missing Values After Treatment:\\n\", missing_values_after_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solving The rest of missing values\n",
    "def fix_missing_ffill(df, col):\n",
    "    df[col] = df[col].fillna(method='ffill')\n",
    "    return df[col]\n",
    "\n",
    "raw_df['Start'] = fix_missing_ffill(raw_df, 'Start')\n",
    "raw_df['End'] = fix_missing_ffill(raw_df, 'End')\n",
    "raw_df['Last Location Name'] = fix_missing_ffill(raw_df, 'Last Location Name')\n",
    "\n",
    "missing_values = raw_df.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the basic metrics (mean, median, etc) from the dataset\n",
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct a Non-Graphical Univariate Analysis by computing dispersion parameters for each quantitative variable.\n",
    "\n",
    "quantitative_variables = []\n",
    "\n",
    "# Iterate over each column in the dataset to find the quantitative variable\n",
    "for column in raw_df.columns:   \n",
    "    if raw_df[column].dtype in [int, float]:\n",
    "        quantitative_variables.append(column)\n",
    "\n",
    "# Solution: By calculating the range or Difference b/n max and min value in each variable\n",
    "for column_name in quantitative_variables:\n",
    "    column_data = raw_df[column_name]\n",
    "    data_range = column_data.max() - column_data.min()\n",
    "    print(\"Range of\", column_name, \":\", data_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct a Graphical Univariate Analysis by identifying the most suitable plotting options for each variable and interpret your findings.\n",
    "# column_name = 'Avg RTT DL (ms)'\n",
    "clean_Data = raw_df.dropna()\n",
    "column_names = clean_Data.columns\n",
    "\n",
    "for column_name in column_names:\n",
    "    column_data = clean_Data[column_name]    \n",
    "    plt.hist(column_data, bins=10)\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of ' + column_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable transformations\n",
    "# Segment the users into top five decile classes based on the total duration for all sessions and compute the total data (DL+UL) per decile class. \n",
    "\n",
    "# Calculate the total duration for all sessions for each user\n",
    "user_total_duration = raw_df.groupby('MSISDN/Number')['Dur. (ms)'].sum()\n",
    "\n",
    "# Segment users into decile classes\n",
    "user_deciles = pd.qcut(user_total_duration, q=10, labels=False, duplicates='drop')\n",
    "\n",
    "# Compute the total data (DL+UL) per decile class\n",
    "data_per_decile = raw_df.groupby(user_deciles)[['Total DL (Bytes)', 'Total UL (Bytes)']].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis – compute a correlation matrix for the following variables and interpret your findings: Social Media data, Google data, Email data, Youtube data, Netflix data, Gaming data, Other data \n",
    "# Select the columns for correlation analysis\n",
    "columns = [\n",
    "    'Social Media DL (Bytes)', 'Social Media UL (Bytes)',\n",
    "    'Google DL (Bytes)', 'Google UL (Bytes)',\n",
    "    'Email DL (Bytes)', 'Email UL (Bytes)',\n",
    "    'Youtube DL (Bytes)', 'Youtube UL (Bytes)',\n",
    "    'Netflix DL (Bytes)', 'Netflix UL (Bytes)',\n",
    "    'Gaming DL (Bytes)', 'Gaming UL (Bytes)',\n",
    "    'Other DL (Bytes)', 'Other UL (Bytes)'\n",
    "]\n",
    "\n",
    "# Create a subset dataframe with the selected columns\n",
    "subset_df = raw_df[columns]\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = subset_df.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  User Engagement analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the above metrics per customer id (MSISDN) and report the top 10 customers per engagement metric\n",
    "\n",
    "# Aggregate metrics per customer\n",
    "aggregated_data = raw_df.groupby('MSISDN/Number').agg({\n",
    "    'Bearer Id': 'nunique',  # Session frequency\n",
    "    'Dur. (ms)': 'sum',  # Session duration\n",
    "    'Total UL (Bytes)': 'sum',  # Upload traffic\n",
    "    'Total DL (Bytes)': 'sum'  # Download traffic\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns for better readability\n",
    "aggregated_data.rename(columns={\n",
    "    'Bearer Id': 'Session Frequency',\n",
    "    'Dur. (ms)': 'Session Duration',\n",
    "    'Total UL (Bytes)': 'Total Upload Traffic',\n",
    "    'Total DL (Bytes)': 'Total Download Traffic'\n",
    "}, inplace=True)\n",
    "\n",
    "# Report the top 10 customers per engagement metric\n",
    "top_10_frequency = aggregated_data.nlargest(10, 'Session Frequency')\n",
    "top_10_duration = aggregated_data.nlargest(10, 'Session Duration')\n",
    "top_10_upload_traffic = aggregated_data.nlargest(10, 'Total Upload Traffic')\n",
    "top_10_download_traffic = aggregated_data.nlargest(10, 'Total Download Traffic')\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 customers by Session Frequency:\")\n",
    "print(top_10_frequency)\n",
    "\n",
    "print(\"\\nTop 10 customers by Session Duration:\")\n",
    "print(top_10_duration)\n",
    "\n",
    "print(\"\\nTop 10 customers by Total Upload Traffic:\")\n",
    "print(top_10_upload_traffic)\n",
    "\n",
    "print(\"\\nTop 10 customers by Total Download Traffic:\")\n",
    "print(top_10_download_traffic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each engagement metric and run a k-means (k=3) to classify customers in three groups of engagement. \n",
    "# Aggregate metrics per customer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "aggregated_data = raw_df.groupby('MSISDN/Number').agg({\n",
    "    'Bearer Id': 'nunique',  # Session frequency\n",
    "    'Dur. (ms)': 'sum',  # Session duration\n",
    "    'Total UL (Bytes)': 'sum',  # Upload traffic\n",
    "    'Total DL (Bytes)': 'sum'  # Download traffic\n",
    "}).reset_index()\n",
    "\n",
    "# Normalize the engagement metrics\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(aggregated_data.iloc[:, 1:])  # Exclude customer ID\n",
    "\n",
    "# Run k-means clustering\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(normalized_data)\n",
    "\n",
    "# Add the cluster labels to the aggregated data\n",
    "aggregated_data['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Report the top customers per cluster\n",
    "top_customers_per_cluster = []\n",
    "for i in range(k):\n",
    "    cluster_customers = aggregated_data[aggregated_data['Cluster'] == i].nlargest(10, 'Bearer Id')\n",
    "    top_customers_per_cluster.append(cluster_customers)\n",
    "\n",
    "# Display the results\n",
    "for i, cluster_customers in enumerate(top_customers_per_cluster):\n",
    "    print(f\"\\nTop 10 customers in Cluster {i+1}:\")\n",
    "    print(cluster_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum, maximum, average & total non-normalized metrics for each cluster. \n",
    "# Interpret your results visually with accompanying text explaining your findings.\n",
    "\n",
    "# Aggregate metrics per customer\n",
    "aggregated_data = raw_df.groupby('MSISDN/Number').agg({\n",
    "    'Bearer Id': 'nunique',  # Session frequency\n",
    "    'Dur. (ms)': 'sum',  # Session duration\n",
    "    'Total UL (Bytes)': 'sum',  # Upload traffic\n",
    "    'Total DL (Bytes)': 'sum'  # Download traffic\n",
    "}).reset_index()\n",
    "\n",
    "# Normalize the engagement metrics\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(aggregated_data.iloc[:, 1:])  # Exclude customer ID\n",
    "\n",
    "# Run k-means clustering\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(normalized_data)\n",
    "\n",
    "# Add the cluster labels to the aggregated data\n",
    "aggregated_data['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Compute non-normalized metrics for each cluster\n",
    "cluster_metrics = aggregated_data.groupby('Cluster').agg({\n",
    "    'Bearer Id': ['min', 'max', 'mean', 'sum'],  # Session frequency\n",
    "    'Dur. (ms)': ['min', 'max', 'mean', 'sum'],  # Session duration\n",
    "    'Total UL (Bytes)': ['min', 'max', 'mean', 'sum'],  # Upload traffic\n",
    "    'Total DL (Bytes)': ['min', 'max', 'mean', 'sum']  # Download traffic\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "print(\"Non-normalized metrics for each cluster:\")\n",
    "print(cluster_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate user total traffic per application and derive the top 10 most engaged users per application\n",
    "\n",
    "# Extract the relevant columns for application traffic\n",
    "app_columns = ['MSISDN/Number', 'Social Media DL (Bytes)', 'Google DL (Bytes)', \n",
    "               'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', \n",
    "               'Gaming DL (Bytes)', 'Other DL (Bytes)']\n",
    "app_traffic = raw_df[app_columns].copy()\n",
    "\n",
    "# Rename the application columns for easier processing\n",
    "app_traffic.columns = ['MSISDN/Number', 'Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']\n",
    "\n",
    "# Melt the dataframe to combine all application columns into a single 'Application' column\n",
    "app_traffic = app_traffic.melt(id_vars='MSISDN/Number', var_name='Application', value_name='Total Traffic')\n",
    "\n",
    "# Aggregate total traffic per application for each user\n",
    "app_traffic = app_traffic.groupby(['MSISDN/Number', 'Application'])['Total Traffic'].sum().reset_index()\n",
    "\n",
    "# Derive the top 10 most engaged users per application\n",
    "top_users_per_app = []\n",
    "unique_apps = app_traffic['Application'].unique()\n",
    "\n",
    "for app in unique_apps:\n",
    "    top_users = app_traffic[app_traffic['Application'] == app].nlargest(10, 'Total Traffic')\n",
    "    top_users_per_app.append(top_users)\n",
    "\n",
    "# Display the results\n",
    "for i, app in enumerate(unique_apps):\n",
    "    print(f\"\\nTop 10 most engaged users for Application '{app}':\")\n",
    "    print(top_users_per_app[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 3 most used applications using appropriate charts. \n",
    "\n",
    "# Extract the relevant columns for application traffic\n",
    "app_columns = ['MSISDN/Number', 'Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']\n",
    "app_traffic = raw_df[app_columns].copy()\n",
    "\n",
    "# Rename the application columns for easier processing\n",
    "app_traffic.columns = ['MSISDN/Number', 'Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']\n",
    "\n",
    "# Melt the dataframe to combine all application columns into a single 'Application' column\n",
    "app_traffic = app_traffic.melt(id_vars='MSISDN/Number', var_name='Application', value_name='Total Traffic')\n",
    "\n",
    "# Aggregate total traffic per application\n",
    "app_traffic = app_traffic.groupby('Application')['Total Traffic'].sum().reset_index()\n",
    "\n",
    "# Sort applications by total traffic in descending order\n",
    "app_traffic = app_traffic.sort_values('Total Traffic', ascending=False)\n",
    "\n",
    "# Select the top 3 most used applications\n",
    "top_3_apps = app_traffic.head(3)\n",
    "\n",
    "# Plot the top 3 most used applications\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(top_3_apps['Application'], top_3_apps['Total Traffic'])\n",
    "plt.xlabel('Application')\n",
    "plt.ylabel('Total Traffic')\n",
    "plt.title('Top 3 Most Used Applications')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using k-means clustering algorithm, group users in k engagement clusters based on the engagement metrics: \n",
    "# What is the optimized value of k (use elbow method for this)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate average TCP retransmission per customer\n",
    "average_tcp_retransmission = raw_df.groupby('MSISDN/Number')['TCP DL Retrans. Vol (Bytes)'].mean()\n",
    "\n",
    "# Print the aggregated results\n",
    "print(\"Average TCP Retransmission per Customer:\")\n",
    "print(average_tcp_retransmission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate average RTT per customer\n",
    "average_rtt = raw_df.groupby('MSISDN/Number')['Avg RTT DL (ms)'].mean()\n",
    "\n",
    "# Print the aggregated results\n",
    "print(\"Average RTT per Customer:\")\n",
    "print(average_rtt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate handset type per customer\n",
    "handset_type = raw_df.groupby('MSISDN/Number')['Handset Type'].first()\n",
    "\n",
    "# Print the aggregated results\n",
    "print(\"Aggregate handset type per Customer:\")\n",
    "print(handset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate average throughput per customer\n",
    "average_throughput = raw_df.groupby('MSISDN/Number')['Avg Bearer TP DL (kbps)'].mean()\n",
    "\n",
    "# Print the aggregated results\n",
    "print(\"Average Throughput per Customer:\")\n",
    "print(average_throughput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute & list 10 of the top, bottom and most frequent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 TCP values\n",
    "top_tcp_values = raw_df['TCP DL Retrans. Vol (Bytes)'].nlargest(10)\n",
    "\n",
    "# Get the bottom 10 TCP values\n",
    "bottom_tcp_values = raw_df['TCP DL Retrans. Vol (Bytes)'].nsmallest(10)\n",
    "\n",
    "# Get the most frequent TCP values\n",
    "most_frequent_tcp_values = raw_df['TCP DL Retrans. Vol (Bytes)'].value_counts().head(10)\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 10 TCP Values:\")\n",
    "print(top_tcp_values)\n",
    "print(\"\\nBottom 10 TCP Values:\")\n",
    "print(bottom_tcp_values)\n",
    "print(\"\\nMost Frequent TCP Values:\")\n",
    "print(most_frequent_tcp_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 RTT values\n",
    "top_rtt_values = raw_df['Avg RTT DL (ms)'].nlargest(10)\n",
    "\n",
    "# Get the bottom 10 RTT values\n",
    "bottom_rtt_values = raw_df['Avg RTT DL (ms)'].nsmallest(10)\n",
    "\n",
    "# Get the most frequent RTT values\n",
    "most_frequent_rtt_values = raw_df['Avg RTT DL (ms)'].value_counts().head(10)\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 10 RTT Values:\")\n",
    "print(top_rtt_values)\n",
    "print(\"\\nBottom 10 RTT Values:\")\n",
    "print(bottom_rtt_values)\n",
    "print(\"\\nMost Frequent RTT Values:\")\n",
    "print(most_frequent_rtt_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 throughput values\n",
    "top_throughput_values = raw_df['Avg Bearer TP DL (kbps)'].nlargest(10)\n",
    "\n",
    "# Get the bottom 10 throughput values\n",
    "bottom_throughput_values = raw_df['Avg Bearer TP DL (kbps)'].nsmallest(10)\n",
    "\n",
    "# Get the most frequent throughput values\n",
    "most_frequent_throughput_values = raw_df['Avg Bearer TP DL (kbps)'].value_counts().head(10)\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 10 Throughput Values:\")\n",
    "print(top_throughput_values)\n",
    "print(\"\\nBottom 10 Throughput Values:\")\n",
    "print(bottom_throughput_values)\n",
    "print(\"\\nMost Frequent Throughput Values:\")\n",
    "print(most_frequent_throughput_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the distribution of average throughput per handset type\n",
    "avg_throughput_distribution = raw_df.groupby('Handset Type')['Avg Bearer TP DL (kbps)'].mean()\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "avg_throughput_distribution.plot(kind='bar')\n",
    "plt.xlabel('Handset Type')\n",
    "plt.ylabel('Average Throughput (kbps)')\n",
    "plt.title('Distribution of Average Throughput per Handset Type')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average TCP retransmission view per handset type\n",
    "avg_tcp_retransmission = raw_df.groupby('Handset Type')['TCP DL Retrans. Vol (Bytes)'].mean()\n",
    "\n",
    "# Plot the average TCP retransmission view per handset type\n",
    "plt.figure(figsize=(12, 6))\n",
    "avg_tcp_retransmission.plot(kind='bar')\n",
    "plt.xlabel('Handset Type')\n",
    "plt.ylabel('Average TCP Retransmission')\n",
    "plt.title('Average TCP Retransmission View per Handset Type')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\o876\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Descriptions:\n",
      "         Avg RTT DL (ms)  Avg Bearer TP DL (kbps)  TCP DL Retrans. Vol (Bytes)\n",
      "Cluster                                                                       \n",
      "0              72.172680              3086.689841                 2.157774e+07\n",
      "1             107.469523             25438.156432                 1.506082e+06\n",
      "2             140.401249             35630.835801                 1.873654e+08\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Select the relevant columns for clustering\n",
    "data = raw_df[['Avg RTT DL (ms)', 'Avg Bearer TP DL (kbps)', 'TCP DL Retrans. Vol (Bytes)']]\n",
    "\n",
    "# Perform k-means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(data)\n",
    "\n",
    "# Add cluster labels to the dataset\n",
    "raw_df['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Describe each cluster\n",
    "cluster_descriptions = raw_df.groupby('Cluster').agg({\n",
    "    'Avg RTT DL (ms)': 'mean',\n",
    "    'Avg Bearer TP DL (kbps)': 'mean',\n",
    "    'TCP DL Retrans. Vol (Bytes)': 'mean'\n",
    "})\n",
    "\n",
    "# Print the cluster descriptions\n",
    "print(\"Cluster Descriptions:\")\n",
    "print(cluster_descriptions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
