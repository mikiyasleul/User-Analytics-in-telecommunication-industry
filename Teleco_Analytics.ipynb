{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('script')\n",
    "from script import dbconn\n",
    "pgconn = dbconn.db_connection_psycopg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\o876\\Desktop\\User-Analytics-in-telecommunication-industry\\script\\dbconn.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = sqlio.read_sql_query(sql,pgconn)\n"
     ]
    }
   ],
   "source": [
    "# Fectching data from the postgreSql database and put the value on raw_df\n",
    "raw_df = dbconn.db_read_table_psycopg(pgconn,'xdr_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Overview analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the top 10 handsets used by the customers.\n",
    "# Count handset usage\n",
    "handset_counts = raw_df.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='Count')\n",
    "\n",
    "# Sort the results\n",
    "sorted_handsets = handset_counts.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Select the top 10 handsets\n",
    "top_10_handsets = sorted_handsets.head(10)\n",
    "\n",
    "# Print the top 10 handsets\n",
    "print(top_10_handsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying Top 3 handset manufacturers\n",
    "manufacturer_counts = raw_df['Handset Manufacturer'].value_counts().reset_index()\n",
    "manufacturer_counts.columns = ['Handset Manufacturer', 'Count']\n",
    "\n",
    "# Sort the results\n",
    "sorted_manufacturers = manufacturer_counts.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Select the top 3 handset manufacturers\n",
    "top_3_manufacturers = sorted_manufacturers.head(3)\n",
    "\n",
    "# Print the top 3 handset manufacturers\n",
    "print(top_3_manufacturers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the top 5 handsets per top 3 handset manufacturer\n",
    "# Count handset manufacturers and types\n",
    "manufacturer_type_counts = raw_df.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='Count')\n",
    "\n",
    "# Sort the results within each manufacturer\n",
    "sorted_manufacturer_types = manufacturer_type_counts.groupby('Handset Manufacturer').apply(lambda x: x.nlargest(5, 'Count')).reset_index(drop=True)\n",
    "\n",
    "# Print the top 5 handsets per top 3 handset manufacturers\n",
    "top_3_manufacturers = sorted_manufacturer_types['Handset Manufacturer'].unique()[:3]\n",
    "\n",
    "for manufacturer in top_3_manufacturers:\n",
    "    print(f\"Top 5 handsets for {manufacturer}:\")\n",
    "    manufacturer_handsets = sorted_manufacturer_types[sorted_manufacturer_types['Handset Manufacturer'] == manufacturer]\n",
    "    print(manufacturer_handsets)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying usersâ€™ behaviour on those Applications / Social Media, Google, Email, Youtube, Netflix, Gaming, Other.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the number of xDR sessions per user\n",
    "sessions_per_user = raw_df.groupby('MSISDN/Number')['Bearer Id'].count()\n",
    "print(sessions_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the session duration per user\n",
    "session_duration_per_user = raw_df.groupby('MSISDN/Number')['Dur. (ms)'].sum()\n",
    "\n",
    "# Display the result\n",
    "print(session_duration_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the total download (Dl) and upload data per user\n",
    "total_data_per_user = raw_df.groupby('MSISDN/Number')[['Total DL (Bytes)', 'Total UL (Bytes)']].sum()\n",
    "\n",
    "# Display the result\n",
    "print(total_data_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of application columns\n",
    "applications = ['Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']\n",
    "\n",
    "# Aggregate the total data volume per user and application\n",
    "total_data_per_user_app = raw_df.groupby('MSISDN/Number')[[col + ' DL (Bytes)' for col in applications] + [col + ' UL (Bytes)' for col in applications]].sum()\n",
    "\n",
    "# Display the result\n",
    "print(total_data_per_user_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanatory Data Analysis | EDA\n",
    "# Treat all missing values and outliers in the dataset by replacing by the mean of the corresponding column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bearer Id                                      991\n",
       "Start                                            1\n",
       "Start ms                                         1\n",
       "End                                              1\n",
       "End ms                                           1\n",
       "Dur. (ms)                                        1\n",
       "IMSI                                           570\n",
       "MSISDN/Number                                 1066\n",
       "IMEI                                           572\n",
       "Last Location Name                            1153\n",
       "Avg RTT DL (ms)                              27829\n",
       "Avg RTT UL (ms)                              27812\n",
       "Avg Bearer TP DL (kbps)                          1\n",
       "Avg Bearer TP UL (kbps)                          1\n",
       "TCP DL Retrans. Vol (Bytes)                  88146\n",
       "TCP UL Retrans. Vol (Bytes)                  96649\n",
       "DL TP < 50 Kbps (%)                            754\n",
       "50 Kbps < DL TP < 250 Kbps (%)                 754\n",
       "250 Kbps < DL TP < 1 Mbps (%)                  754\n",
       "DL TP > 1 Mbps (%)                             754\n",
       "UL TP < 10 Kbps (%)                            792\n",
       "10 Kbps < UL TP < 50 Kbps (%)                  792\n",
       "50 Kbps < UL TP < 300 Kbps (%)                 792\n",
       "UL TP > 300 Kbps (%)                           792\n",
       "HTTP DL (Bytes)                              81474\n",
       "HTTP UL (Bytes)                              81810\n",
       "Activity Duration DL (ms)                        1\n",
       "Activity Duration UL (ms)                        1\n",
       "Dur. (ms).1                                      1\n",
       "Handset Manufacturer                           572\n",
       "Handset Type                                   572\n",
       "Nb of sec with 125000B < Vol DL              97538\n",
       "Nb of sec with 1250B < Vol UL < 6250B        92894\n",
       "Nb of sec with 31250B < Vol DL < 125000B     93586\n",
       "Nb of sec with 37500B < Vol UL              130254\n",
       "Nb of sec with 6250B < Vol DL < 31250B       88317\n",
       "Nb of sec with 6250B < Vol UL < 37500B      111843\n",
       "Nb of sec with Vol DL < 6250B                  755\n",
       "Nb of sec with Vol UL < 1250B                  793\n",
       "Social Media DL (Bytes)                          0\n",
       "Social Media UL (Bytes)                          0\n",
       "Google DL (Bytes)                                0\n",
       "Google UL (Bytes)                                0\n",
       "Email DL (Bytes)                                 0\n",
       "Email UL (Bytes)                                 0\n",
       "Youtube DL (Bytes)                               0\n",
       "Youtube UL (Bytes)                               0\n",
       "Netflix DL (Bytes)                               0\n",
       "Netflix UL (Bytes)                               0\n",
       "Gaming DL (Bytes)                                0\n",
       "Gaming UL (Bytes)                                0\n",
       "Other DL (Bytes)                                 0\n",
       "Other UL (Bytes)                                 0\n",
       "Total UL (Bytes)                                 1\n",
       "Total DL (Bytes)                                 1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\o876\\AppData\\Local\\Temp\\ipykernel_6540\\748560962.py:17: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  percent_missing(raw_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 12.5 % missing values.\n"
     ]
    }
   ],
   "source": [
    "#percent of missing data\n",
    "def percent_missing(df):\n",
    "    # Calculate total number of cells in dataframe\n",
    "    totalCells = np.product(df.shape)\n",
    "\n",
    "    # Count number of missing values per column\n",
    "    missingCount = df.isnull().sum()\n",
    "\n",
    "    # Calculate total number of missing values\n",
    "    totalMissing = missingCount.sum()\n",
    "\n",
    "    # Calculate percentage of missing values\n",
    "    percentageMissing = (totalMissing / totalCells) * 100\n",
    "\n",
    "    print(\"The dataset contains\", round(percentageMissing, 2), \"%\", \"missing values.\")\n",
    "\n",
    "percent_missing(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values After Treatment:\n",
      " Bearer Id                                   0\n",
      "Start                                       0\n",
      "Start ms                                    0\n",
      "End                                         0\n",
      "End ms                                      0\n",
      "Dur. (ms)                                   0\n",
      "IMSI                                        0\n",
      "MSISDN/Number                               0\n",
      "IMEI                                        0\n",
      "Last Location Name                          0\n",
      "Avg RTT DL (ms)                             0\n",
      "Avg RTT UL (ms)                             0\n",
      "Avg Bearer TP DL (kbps)                     0\n",
      "Avg Bearer TP UL (kbps)                     0\n",
      "TCP DL Retrans. Vol (Bytes)                 0\n",
      "TCP UL Retrans. Vol (Bytes)                 0\n",
      "DL TP < 50 Kbps (%)                         0\n",
      "50 Kbps < DL TP < 250 Kbps (%)              0\n",
      "250 Kbps < DL TP < 1 Mbps (%)               0\n",
      "DL TP > 1 Mbps (%)                          0\n",
      "UL TP < 10 Kbps (%)                         0\n",
      "10 Kbps < UL TP < 50 Kbps (%)               0\n",
      "50 Kbps < UL TP < 300 Kbps (%)              0\n",
      "UL TP > 300 Kbps (%)                        0\n",
      "HTTP DL (Bytes)                             0\n",
      "HTTP UL (Bytes)                             0\n",
      "Activity Duration DL (ms)                   0\n",
      "Activity Duration UL (ms)                   0\n",
      "Dur. (ms).1                                 0\n",
      "Handset Manufacturer                        0\n",
      "Handset Type                                0\n",
      "Nb of sec with 125000B < Vol DL             0\n",
      "Nb of sec with 1250B < Vol UL < 6250B       0\n",
      "Nb of sec with 31250B < Vol DL < 125000B    0\n",
      "Nb of sec with 37500B < Vol UL              0\n",
      "Nb of sec with 6250B < Vol DL < 31250B      0\n",
      "Nb of sec with 6250B < Vol UL < 37500B      0\n",
      "Nb of sec with Vol DL < 6250B               0\n",
      "Nb of sec with Vol UL < 1250B               0\n",
      "Social Media DL (Bytes)                     0\n",
      "Social Media UL (Bytes)                     0\n",
      "Google DL (Bytes)                           0\n",
      "Google UL (Bytes)                           0\n",
      "Email DL (Bytes)                            0\n",
      "Email UL (Bytes)                            0\n",
      "Youtube DL (Bytes)                          0\n",
      "Youtube UL (Bytes)                          0\n",
      "Netflix DL (Bytes)                          0\n",
      "Netflix UL (Bytes)                          0\n",
      "Gaming DL (Bytes)                           0\n",
      "Gaming UL (Bytes)                           0\n",
      "Other DL (Bytes)                            0\n",
      "Other UL (Bytes)                            0\n",
      "Total UL (Bytes)                            0\n",
      "Total DL (Bytes)                            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replace missing values with column mean for numeric columns\n",
    "numeric_columns = raw_df.select_dtypes(include=[np.number]).columns\n",
    "raw_df[numeric_columns] = raw_df[numeric_columns].fillna(raw_df[numeric_columns].mean())\n",
    "\n",
    "# Replace missing values with column mode for non-numeric columns\n",
    "non_numeric_columns = raw_df.select_dtypes(exclude=[np.number]).columns\n",
    "raw_df[non_numeric_columns] = raw_df[non_numeric_columns].fillna(raw_df[non_numeric_columns].mode().iloc[0])\n",
    "\n",
    "# Identify and replace outliers with column mean for numeric columns\n",
    "for col in numeric_columns:\n",
    "    z_scores = (raw_df[col] - raw_df[col].mean()) / raw_df[col].std()\n",
    "    outliers = (z_scores > 3) | (z_scores < -3)\n",
    "    raw_df.loc[outliers, col] = raw_df[col].mean()\n",
    "\n",
    "# Verify missing values and outliers have been treated\n",
    "missing_values_after_treatment = raw_df.isnull().sum()\n",
    "print(\"Missing Values After Treatment:\\n\", missing_values_after_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\o876\\AppData\\Local\\Temp\\ipykernel_2612\\3212002553.py:3: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[col] = df[col].fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bearer Id                                   0\n",
      "Start                                       0\n",
      "Start ms                                    0\n",
      "End                                         0\n",
      "End ms                                      0\n",
      "Dur. (ms)                                   0\n",
      "IMSI                                        0\n",
      "MSISDN/Number                               0\n",
      "IMEI                                        0\n",
      "Last Location Name                          0\n",
      "Avg RTT DL (ms)                             0\n",
      "Avg RTT UL (ms)                             0\n",
      "Avg Bearer TP DL (kbps)                     0\n",
      "Avg Bearer TP UL (kbps)                     0\n",
      "TCP DL Retrans. Vol (Bytes)                 0\n",
      "TCP UL Retrans. Vol (Bytes)                 0\n",
      "DL TP < 50 Kbps (%)                         0\n",
      "50 Kbps < DL TP < 250 Kbps (%)              0\n",
      "250 Kbps < DL TP < 1 Mbps (%)               0\n",
      "DL TP > 1 Mbps (%)                          0\n",
      "UL TP < 10 Kbps (%)                         0\n",
      "10 Kbps < UL TP < 50 Kbps (%)               0\n",
      "50 Kbps < UL TP < 300 Kbps (%)              0\n",
      "UL TP > 300 Kbps (%)                        0\n",
      "HTTP DL (Bytes)                             0\n",
      "HTTP UL (Bytes)                             0\n",
      "Activity Duration DL (ms)                   0\n",
      "Activity Duration UL (ms)                   0\n",
      "Dur. (ms).1                                 0\n",
      "Handset Manufacturer                        0\n",
      "Handset Type                                0\n",
      "Nb of sec with 125000B < Vol DL             0\n",
      "Nb of sec with 1250B < Vol UL < 6250B       0\n",
      "Nb of sec with 31250B < Vol DL < 125000B    0\n",
      "Nb of sec with 37500B < Vol UL              0\n",
      "Nb of sec with 6250B < Vol DL < 31250B      0\n",
      "Nb of sec with 6250B < Vol UL < 37500B      0\n",
      "Nb of sec with Vol DL < 6250B               0\n",
      "Nb of sec with Vol UL < 1250B               0\n",
      "Social Media DL (Bytes)                     0\n",
      "Social Media UL (Bytes)                     0\n",
      "Google DL (Bytes)                           0\n",
      "Google UL (Bytes)                           0\n",
      "Email DL (Bytes)                            0\n",
      "Email UL (Bytes)                            0\n",
      "Youtube DL (Bytes)                          0\n",
      "Youtube UL (Bytes)                          0\n",
      "Netflix DL (Bytes)                          0\n",
      "Netflix UL (Bytes)                          0\n",
      "Gaming DL (Bytes)                           0\n",
      "Gaming UL (Bytes)                           0\n",
      "Other DL (Bytes)                            0\n",
      "Other UL (Bytes)                            0\n",
      "Total UL (Bytes)                            0\n",
      "Total DL (Bytes)                            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Solving The rest of missing values\n",
    "def fix_missing_ffill(df, col):\n",
    "    df[col] = df[col].fillna(method='ffill')\n",
    "    return df[col]\n",
    "\n",
    "raw_df['Start'] = fix_missing_ffill(raw_df, 'Start')\n",
    "raw_df['End'] = fix_missing_ffill(raw_df, 'End')\n",
    "raw_df['Last Location Name'] = fix_missing_ffill(raw_df, 'Last Location Name')\n",
    "\n",
    "missing_values = raw_df.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the basic metrics (mean, median, etc) from the dataset\n",
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct a Non-Graphical Univariate Analysis by computing dispersion parameters for each quantitative variable.\n",
    "\n",
    "quantitative_variables = []\n",
    "\n",
    "# Iterate over each column in the dataset to find the quantitative variable\n",
    "for column in raw_df.columns:   \n",
    "    if raw_df[column].dtype in [int, float]:\n",
    "        quantitative_variables.append(column)\n",
    "\n",
    "# Solution: By calculating the range or Difference b/n max and min value in each variable\n",
    "for column_name in quantitative_variables:\n",
    "    column_data = raw_df[column_name]\n",
    "    data_range = column_data.max() - column_data.min()\n",
    "    print(\"Range of\", column_name, \":\", data_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct a Graphical Univariate Analysis by identifying the most suitable plotting options for each variable and interpret your findings.\n",
    "# column_name = 'Avg RTT DL (ms)'\n",
    "clean_Data = raw_df.dropna()\n",
    "column_names = clean_Data.columns\n",
    "\n",
    "for column_name in column_names:\n",
    "    column_data = clean_Data[column_name]    \n",
    "    plt.hist(column_data, bins=10)\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of ' + column_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable transformations\n",
    "# Segment the users into top five decile classes based on the total duration for all sessions and compute the total data (DL+UL) per decile class. \n",
    "\n",
    "# Calculate the total duration for all sessions for each user\n",
    "user_total_duration = raw_df.groupby('MSISDN/Number')['Dur. (ms)'].sum()\n",
    "\n",
    "# Segment users into decile classes\n",
    "user_deciles = pd.qcut(user_total_duration, q=10, labels=False, duplicates='drop')\n",
    "\n",
    "# Compute the total data (DL+UL) per decile class\n",
    "data_per_decile = raw_df.groupby(user_deciles)[['Total DL (Bytes)', 'Total UL (Bytes)']].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis â€“ compute a correlation matrix for the following variables and interpret your findings: Social Media data, Google data, Email data, Youtube data, Netflix data, Gaming data, Other data \n",
    "# Select the columns for correlation analysis\n",
    "columns = [\n",
    "    'Social Media DL (Bytes)', 'Social Media UL (Bytes)',\n",
    "    'Google DL (Bytes)', 'Google UL (Bytes)',\n",
    "    'Email DL (Bytes)', 'Email UL (Bytes)',\n",
    "    'Youtube DL (Bytes)', 'Youtube UL (Bytes)',\n",
    "    'Netflix DL (Bytes)', 'Netflix UL (Bytes)',\n",
    "    'Gaming DL (Bytes)', 'Gaming UL (Bytes)',\n",
    "    'Other DL (Bytes)', 'Other UL (Bytes)'\n",
    "]\n",
    "\n",
    "# Create a subset dataframe with the selected columns\n",
    "subset_df = raw_df[columns]\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = subset_df.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  User Engagement analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the above metrics per customer id (MSISDN) and report the top 10 customers per engagement metric\n",
    "\n",
    "# Aggregate metrics per customer\n",
    "aggregated_data = raw_df.groupby('MSISDN/Number').agg({\n",
    "    'Bearer Id': 'nunique',  # Session frequency\n",
    "    'Dur. (ms)': 'sum',  # Session duration\n",
    "    'Total UL (Bytes)': 'sum',  # Upload traffic\n",
    "    'Total DL (Bytes)': 'sum'  # Download traffic\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns for better readability\n",
    "aggregated_data.rename(columns={\n",
    "    'Bearer Id': 'Session Frequency',\n",
    "    'Dur. (ms)': 'Session Duration',\n",
    "    'Total UL (Bytes)': 'Total Upload Traffic',\n",
    "    'Total DL (Bytes)': 'Total Download Traffic'\n",
    "}, inplace=True)\n",
    "\n",
    "# Report the top 10 customers per engagement metric\n",
    "top_10_frequency = aggregated_data.nlargest(10, 'Session Frequency')\n",
    "top_10_duration = aggregated_data.nlargest(10, 'Session Duration')\n",
    "top_10_upload_traffic = aggregated_data.nlargest(10, 'Total Upload Traffic')\n",
    "top_10_download_traffic = aggregated_data.nlargest(10, 'Total Download Traffic')\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 customers by Session Frequency:\")\n",
    "print(top_10_frequency)\n",
    "\n",
    "print(\"\\nTop 10 customers by Session Duration:\")\n",
    "print(top_10_duration)\n",
    "\n",
    "print(\"\\nTop 10 customers by Total Upload Traffic:\")\n",
    "print(top_10_upload_traffic)\n",
    "\n",
    "print(\"\\nTop 10 customers by Total Download Traffic:\")\n",
    "print(top_10_download_traffic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each engagement metric and run a k-means (k=3) to classify customers in three groups of engagement. \n",
    "# Aggregate metrics per customer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "aggregated_data = raw_df.groupby('MSISDN/Number').agg({\n",
    "    'Bearer Id': 'nunique',  # Session frequency\n",
    "    'Dur. (ms)': 'sum',  # Session duration\n",
    "    'Total UL (Bytes)': 'sum',  # Upload traffic\n",
    "    'Total DL (Bytes)': 'sum'  # Download traffic\n",
    "}).reset_index()\n",
    "\n",
    "# Normalize the engagement metrics\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(aggregated_data.iloc[:, 1:])  # Exclude customer ID\n",
    "\n",
    "# Run k-means clustering\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(normalized_data)\n",
    "\n",
    "# Add the cluster labels to the aggregated data\n",
    "aggregated_data['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Report the top customers per cluster\n",
    "top_customers_per_cluster = []\n",
    "for i in range(k):\n",
    "    cluster_customers = aggregated_data[aggregated_data['Cluster'] == i].nlargest(10, 'Bearer Id')\n",
    "    top_customers_per_cluster.append(cluster_customers)\n",
    "\n",
    "# Display the results\n",
    "for i, cluster_customers in enumerate(top_customers_per_cluster):\n",
    "    print(f\"\\nTop 10 customers in Cluster {i+1}:\")\n",
    "    print(cluster_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum, maximum, average & total non-normalized metrics for each cluster. \n",
    "# Interpret your results visually with accompanying text explaining your findings.\n",
    "\n",
    "# Aggregate metrics per customer\n",
    "aggregated_data = raw_df.groupby('MSISDN/Number').agg({\n",
    "    'Bearer Id': 'nunique',  # Session frequency\n",
    "    'Dur. (ms)': 'sum',  # Session duration\n",
    "    'Total UL (Bytes)': 'sum',  # Upload traffic\n",
    "    'Total DL (Bytes)': 'sum'  # Download traffic\n",
    "}).reset_index()\n",
    "\n",
    "# Normalize the engagement metrics\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(aggregated_data.iloc[:, 1:])  # Exclude customer ID\n",
    "\n",
    "# Run k-means clustering\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(normalized_data)\n",
    "\n",
    "# Add the cluster labels to the aggregated data\n",
    "aggregated_data['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Compute non-normalized metrics for each cluster\n",
    "cluster_metrics = aggregated_data.groupby('Cluster').agg({\n",
    "    'Bearer Id': ['min', 'max', 'mean', 'sum'],  # Session frequency\n",
    "    'Dur. (ms)': ['min', 'max', 'mean', 'sum'],  # Session duration\n",
    "    'Total UL (Bytes)': ['min', 'max', 'mean', 'sum'],  # Upload traffic\n",
    "    'Total DL (Bytes)': ['min', 'max', 'mean', 'sum']  # Download traffic\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "print(\"Non-normalized metrics for each cluster:\")\n",
    "print(cluster_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate user total traffic per application and derive the top 10 most engaged users per application\n",
    "\n",
    "# Extract the relevant columns for application traffic\n",
    "app_columns = ['MSISDN/Number', 'Social Media DL (Bytes)', 'Google DL (Bytes)', \n",
    "               'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', \n",
    "               'Gaming DL (Bytes)', 'Other DL (Bytes)']\n",
    "app_traffic = raw_df[app_columns].copy()\n",
    "\n",
    "# Rename the application columns for easier processing\n",
    "app_traffic.columns = ['MSISDN/Number', 'Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']\n",
    "\n",
    "# Melt the dataframe to combine all application columns into a single 'Application' column\n",
    "app_traffic = app_traffic.melt(id_vars='MSISDN/Number', var_name='Application', value_name='Total Traffic')\n",
    "\n",
    "# Aggregate total traffic per application for each user\n",
    "app_traffic = app_traffic.groupby(['MSISDN/Number', 'Application'])['Total Traffic'].sum().reset_index()\n",
    "\n",
    "# Derive the top 10 most engaged users per application\n",
    "top_users_per_app = []\n",
    "unique_apps = app_traffic['Application'].unique()\n",
    "\n",
    "for app in unique_apps:\n",
    "    top_users = app_traffic[app_traffic['Application'] == app].nlargest(10, 'Total Traffic')\n",
    "    top_users_per_app.append(top_users)\n",
    "\n",
    "# Display the results\n",
    "for i, app in enumerate(unique_apps):\n",
    "    print(f\"\\nTop 10 most engaged users for Application '{app}':\")\n",
    "    print(top_users_per_app[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 3 most used applications using appropriate charts. \n",
    "\n",
    "# Extract the relevant columns for application traffic\n",
    "app_columns = ['MSISDN/Number', 'Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']\n",
    "app_traffic = raw_df[app_columns].copy()\n",
    "\n",
    "# Rename the application columns for easier processing\n",
    "app_traffic.columns = ['MSISDN/Number', 'Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']\n",
    "\n",
    "# Melt the dataframe to combine all application columns into a single 'Application' column\n",
    "app_traffic = app_traffic.melt(id_vars='MSISDN/Number', var_name='Application', value_name='Total Traffic')\n",
    "\n",
    "# Aggregate total traffic per application\n",
    "app_traffic = app_traffic.groupby('Application')['Total Traffic'].sum().reset_index()\n",
    "\n",
    "# Sort applications by total traffic in descending order\n",
    "app_traffic = app_traffic.sort_values('Total Traffic', ascending=False)\n",
    "\n",
    "# Select the top 3 most used applications\n",
    "top_3_apps = app_traffic.head(3)\n",
    "\n",
    "# Plot the top 3 most used applications\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(top_3_apps['Application'], top_3_apps['Total Traffic'])\n",
    "plt.xlabel('Application')\n",
    "plt.ylabel('Total Traffic')\n",
    "plt.title('Top 3 Most Used Applications')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using k-means clustering algorithm, group users in k engagement clusters based on the engagement metrics: \n",
    "# What is the optimized value of k (use elbow method for this)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate average TCP retransmission per customer\n",
    "average_tcp_retransmission = raw_df.groupby('MSISDN/Number')['TCP DL Retrans. Vol (Bytes)'].mean()\n",
    "\n",
    "# Print the aggregated results\n",
    "print(\"Average TCP Retransmission per Customer:\")\n",
    "print(average_tcp_retransmission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate average RTT per customer\n",
    "average_rtt = raw_df.groupby('MSISDN/Number')['Avg RTT DL (ms)'].mean()\n",
    "\n",
    "# Print the aggregated results\n",
    "print(\"Average RTT per Customer:\")\n",
    "print(average_rtt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate handset type per customer\n",
    "handset_type = raw_df.groupby('MSISDN/Number')['Handset Type'].first()\n",
    "\n",
    "# Print the aggregated results\n",
    "print(\"Aggregate handset type per Customer:\")\n",
    "print(handset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate average throughput per customer\n",
    "average_throughput = raw_df.groupby('MSISDN/Number')['Avg Bearer TP DL (kbps)'].mean()\n",
    "\n",
    "# Print the aggregated results\n",
    "print(\"Average Throughput per Customer:\")\n",
    "print(average_throughput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute & list 10 of the top, bottom and most frequent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 TCP values\n",
    "top_tcp_values = raw_df['TCP DL Retrans. Vol (Bytes)'].nlargest(10)\n",
    "\n",
    "# Get the bottom 10 TCP values\n",
    "bottom_tcp_values = raw_df['TCP DL Retrans. Vol (Bytes)'].nsmallest(10)\n",
    "\n",
    "# Get the most frequent TCP values\n",
    "most_frequent_tcp_values = raw_df['TCP DL Retrans. Vol (Bytes)'].value_counts().head(10)\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 10 TCP Values:\")\n",
    "print(top_tcp_values)\n",
    "print(\"\\nBottom 10 TCP Values:\")\n",
    "print(bottom_tcp_values)\n",
    "print(\"\\nMost Frequent TCP Values:\")\n",
    "print(most_frequent_tcp_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 RTT values\n",
    "top_rtt_values = raw_df['Avg RTT DL (ms)'].nlargest(10)\n",
    "\n",
    "# Get the bottom 10 RTT values\n",
    "bottom_rtt_values = raw_df['Avg RTT DL (ms)'].nsmallest(10)\n",
    "\n",
    "# Get the most frequent RTT values\n",
    "most_frequent_rtt_values = raw_df['Avg RTT DL (ms)'].value_counts().head(10)\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 10 RTT Values:\")\n",
    "print(top_rtt_values)\n",
    "print(\"\\nBottom 10 RTT Values:\")\n",
    "print(bottom_rtt_values)\n",
    "print(\"\\nMost Frequent RTT Values:\")\n",
    "print(most_frequent_rtt_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 throughput values\n",
    "top_throughput_values = raw_df['Avg Bearer TP DL (kbps)'].nlargest(10)\n",
    "\n",
    "# Get the bottom 10 throughput values\n",
    "bottom_throughput_values = raw_df['Avg Bearer TP DL (kbps)'].nsmallest(10)\n",
    "\n",
    "# Get the most frequent throughput values\n",
    "most_frequent_throughput_values = raw_df['Avg Bearer TP DL (kbps)'].value_counts().head(10)\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 10 Throughput Values:\")\n",
    "print(top_throughput_values)\n",
    "print(\"\\nBottom 10 Throughput Values:\")\n",
    "print(bottom_throughput_values)\n",
    "print(\"\\nMost Frequent Throughput Values:\")\n",
    "print(most_frequent_throughput_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the distribution of average throughput per handset type\n",
    "avg_throughput_distribution = raw_df.groupby('Handset Type')['Avg Bearer TP DL (kbps)'].mean()\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "avg_throughput_distribution.plot(kind='bar')\n",
    "plt.xlabel('Handset Type')\n",
    "plt.ylabel('Average Throughput (kbps)')\n",
    "plt.title('Distribution of Average Throughput per Handset Type')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average TCP retransmission view per handset type\n",
    "avg_tcp_retransmission = raw_df.groupby('Handset Type')['TCP DL Retrans. Vol (Bytes)'].mean()\n",
    "\n",
    "# Plot the average TCP retransmission view per handset type\n",
    "plt.figure(figsize=(12, 6))\n",
    "avg_tcp_retransmission.plot(kind='bar')\n",
    "plt.xlabel('Handset Type')\n",
    "plt.ylabel('Average TCP Retransmission')\n",
    "plt.title('Average TCP Retransmission View per Handset Type')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Select the relevant columns for clustering\n",
    "data = raw_df[['Avg RTT DL (ms)', 'Avg Bearer TP DL (kbps)', 'TCP DL Retrans. Vol (Bytes)']]\n",
    "\n",
    "# Perform k-means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(data)\n",
    "\n",
    "# Add cluster labels to the dataset\n",
    "raw_df['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Describe each cluster\n",
    "cluster_descriptions = raw_df.groupby('Cluster').agg({\n",
    "    'Avg RTT DL (ms)': 'mean',\n",
    "    'Avg Bearer TP DL (kbps)': 'mean',\n",
    "    'TCP DL Retrans. Vol (Bytes)': 'mean'\n",
    "})\n",
    "\n",
    "# Print the cluster descriptions\n",
    "print(\"Cluster Descriptions:\")\n",
    "print(cluster_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Satisfaction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\o876\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 least engaged users:\n",
      "        MSISDN/Number  Cluster  Engagement Score\n",
      "16009    3.360985e+10        1          0.068658\n",
      "148295   3.369948e+10        2          0.144754\n",
      "117786   3.366856e+10        2          0.156107\n",
      "71508    3.366376e+10        0          0.161020\n",
      "115538   3.365013e+10        0          0.161214\n",
      "35951    3.366480e+10        1          0.164404\n",
      "34585    3.366612e+10        1          0.164657\n",
      "127173   3.366355e+10        2          0.168642\n",
      "71466    3.376233e+10        2          0.168886\n",
      "34146    3.366444e+10        1          0.169006\n"
     ]
    }
   ],
   "source": [
    "# Engagement score to each user. Consider the engagement score as the Euclidean distance between the user data point & \n",
    "# the less engaged cluster (use the first clustering for this) (Euclidean Distance)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "engagement_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']\n",
    "engagement_data = raw_df[engagement_columns]\n",
    "\n",
    "# Perform clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42).fit(MinMaxScaler().fit_transform(engagement_data))\n",
    "raw_df['Cluster'] = kmeans.predict(MinMaxScaler().fit_transform(engagement_data))\n",
    "\n",
    "# Compute engagement scores\n",
    "raw_df['Engagement Score'] = kmeans.transform(MinMaxScaler().fit_transform(engagement_data)).min(axis=1)\n",
    "\n",
    "# Display the top 10 users with the lowest engagement scores\n",
    "top_10_least_engaged_users = raw_df.nsmallest(10, 'Engagement Score')\n",
    "print(\"Top 10 least engaged users:\")\n",
    "print(top_10_least_engaged_users[['MSISDN/Number', 'Cluster', 'Engagement Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\o876\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 users with the worst experience:\n",
      "        MSISDN/Number  Cluster  Experience Score\n",
      "16009    3.360985e+10        1          0.068658\n",
      "148295   3.369948e+10        2          0.144754\n",
      "117786   3.366856e+10        2          0.156107\n",
      "71508    3.366376e+10        0          0.161020\n",
      "115538   3.365013e+10        0          0.161214\n",
      "35951    3.366480e+10        1          0.164404\n",
      "34585    3.366612e+10        1          0.164657\n",
      "127173   3.366355e+10        2          0.168642\n",
      "71466    3.376233e+10        2          0.168886\n",
      "34146    3.366444e+10        1          0.169006\n"
     ]
    }
   ],
   "source": [
    "# Experience score to each user. Consider the experience score as the Euclidean distance between the user data point & the worst experienceâ€™s cluster. \n",
    "\n",
    "engagement_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']\n",
    "engagement_data = raw_df[engagement_columns]\n",
    "\n",
    "# Perform clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42).fit(MinMaxScaler().fit_transform(engagement_data))\n",
    "raw_df['Cluster'] = kmeans.predict(MinMaxScaler().fit_transform(engagement_data))\n",
    "\n",
    "# Compute the worst experience cluster\n",
    "worst_experience_cluster = np.argmin(kmeans.inertia_)\n",
    "worst_experience_cluster_centroid = kmeans.cluster_centers_[worst_experience_cluster]\n",
    "\n",
    "# Compute experience scores\n",
    "raw_df['Experience Score'] = kmeans.transform(MinMaxScaler().fit_transform(engagement_data)).min(axis=1)\n",
    "\n",
    "# Display the top 10 users with the worst experience scores\n",
    "top_10_worst_experience_users = raw_df.nsmallest(10, 'Experience Score')\n",
    "print(\"Top 10 users with the worst experience:\")\n",
    "print(top_10_worst_experience_users[['MSISDN/Number', 'Cluster', 'Experience Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\o876\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\o876\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 satisfied customers:\n",
      "        MSISDN/Number  Satisfaction Score\n",
      "124713   3.369943e+10            1.129860\n",
      "43612    3.363233e+10            1.128138\n",
      "38648    3.367117e+10            1.120422\n",
      "35417    3.366736e+10            1.118322\n",
      "143967   3.378923e+10            1.113666\n",
      "84003    3.365011e+10            1.110356\n",
      "132887   3.366400e+10            1.106980\n",
      "1568     3.375586e+10            1.102493\n",
      "30080    3.366728e+10            1.102431\n",
      "110543   3.366297e+10            1.102149\n"
     ]
    }
   ],
   "source": [
    "# Consider the average of both engagement & experience scores as  the satisfaction score & report the top 10 satisfied customer\n",
    "\n",
    "# Calculate engagement scores\n",
    "engagement_data = raw_df[['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']]\n",
    "engagement_scores = KMeans(n_clusters=3, random_state=42).fit_transform(MinMaxScaler().fit_transform(engagement_data)).min(axis=1)\n",
    "\n",
    "# Calculate experience scores\n",
    "experience_scores = KMeans(n_clusters=3, random_state=42).fit_transform(MinMaxScaler().fit_transform(engagement_data)).min(axis=1)\n",
    "\n",
    "# Calculate satisfaction scores as the average of engagement and experience scores\n",
    "satisfaction_scores = (engagement_scores + experience_scores) / 2\n",
    "\n",
    "# Add satisfaction scores to the DataFrame\n",
    "raw_df['Satisfaction Score'] = satisfaction_scores\n",
    "\n",
    "# Report the top 10 satisfied customers\n",
    "top_10_satisfied_customers = raw_df.nlargest(10, 'Satisfaction Score')\n",
    "print(\"Top 10 satisfied customers:\")\n",
    "print(top_10_satisfied_customers[['MSISDN/Number', 'Satisfaction Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.0010827734241375883\n",
      "R-squared (R2) Score: 0.9365999833474234\n"
     ]
    }
   ],
   "source": [
    "# Build a regression model of your choice to predict the satisfaction score of a customer. \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Prepare the features and target variables\n",
    "features = raw_df[['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']]\n",
    "target = raw_df['Satisfaction Score']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Random Forest regression model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the model's performance metrics\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R-squared (R2) Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\o876\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engagement Clusters:\n",
      "Engagement Cluster\n",
      "0    75361\n",
      "1    74640\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Experience Scores:\n",
      "0         0.726574\n",
      "1         0.623798\n",
      "2         0.552089\n",
      "3         0.758693\n",
      "4         0.439322\n",
      "            ...   \n",
      "149996    0.980796\n",
      "149997    0.604966\n",
      "149998    0.649726\n",
      "149999    0.708617\n",
      "150000    0.249494\n",
      "Name: Experience Score, Length: 150001, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Run a k-means (k=2) on the engagement & the experience score.\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Prepare the engagement and experience scores\n",
    "engagement_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']\n",
    "engagement_data = raw_df[engagement_columns]\n",
    "\n",
    "# Scale the engagement data\n",
    "scaler = MinMaxScaler().fit(engagement_data)\n",
    "scaled_engagement_data = scaler.transform(engagement_data)\n",
    "\n",
    "# Run k-means clustering with k=2\n",
    "kmeans = KMeans(n_clusters=2, random_state=42).fit(scaled_engagement_data)\n",
    "\n",
    "# Get the cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Add the cluster labels to the DataFrame\n",
    "raw_df['Engagement Cluster'] = cluster_labels\n",
    "\n",
    "# Compute experience scores\n",
    "experience_scores = kmeans.transform(scaled_engagement_data).min(axis=1)\n",
    "\n",
    "# Add experience scores to the DataFrame\n",
    "raw_df['Experience Score'] = experience_scores\n",
    "\n",
    "# Display the clusters and experience scores\n",
    "print(\"Engagement Clusters:\")\n",
    "print(raw_df['Engagement Cluster'].value_counts())\n",
    "print(\"\\nExperience Scores:\")\n",
    "print(raw_df['Experience Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Satisfaction Score  Experience Score\n",
      "Engagement Cluster                                      \n",
      "0                             0.682653          0.710019\n",
      "1                             0.682582          0.709398\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the average satisfaction & experience score per cluster. \n",
    "cluster_agg = raw_df.groupby('Engagement Cluster').agg({'Satisfaction Score': 'mean', 'Experience Score': 'mean'})\n",
    "print(cluster_agg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
