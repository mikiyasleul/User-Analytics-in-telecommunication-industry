{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('script')\n",
    "from script import dbconn\n",
    "pgconn = dbconn.db_connection_psycopg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\o876\\Desktop\\User-Analytics-in-telecommunication-industry\\script\\dbconn.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = sqlio.read_sql_query(sql,pgconn)\n"
     ]
    }
   ],
   "source": [
    "# Fectching data from the postgreSql database and put the value on raw_df\n",
    "raw_df = dbconn.db_read_table_psycopg(pgconn,'xdr_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Overview analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Handset Manufacturer                  Handset Type  Count\n",
      "316                Huawei              Huawei B528S-23A  19752\n",
      "60                  Apple       Apple iPhone 6S (A1688)   9419\n",
      "56                  Apple        Apple iPhone 6 (A1586)   9023\n",
      "1395            undefined                     undefined   8987\n",
      "66                  Apple        Apple iPhone 7 (A1778)   6326\n",
      "80                  Apple       Apple iPhone Se (A1723)   5187\n",
      "73                  Apple        Apple iPhone 8 (A1905)   4993\n",
      "85                  Apple       Apple iPhone Xr (A2105)   4568\n",
      "957               Samsung  Samsung Galaxy S8 (Sm-G950F)   4520\n",
      "82                  Apple        Apple iPhone X (A1901)   3813\n"
     ]
    }
   ],
   "source": [
    "# Identifying the top 10 handsets used by the customers.\n",
    "# Count handset usage\n",
    "handset_counts = raw_df.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='Count')\n",
    "\n",
    "# Sort the results\n",
    "sorted_handsets = handset_counts.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Select the top 10 handsets\n",
    "top_10_handsets = sorted_handsets.head(10)\n",
    "\n",
    "# Print the top 10 handsets\n",
    "print(top_10_handsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Handset Manufacturer  Count\n",
      "0                Apple  59565\n",
      "1              Samsung  40839\n",
      "2               Huawei  34423\n"
     ]
    }
   ],
   "source": [
    "# Identifying Top 3 handset manufacturers\n",
    "manufacturer_counts = raw_df['Handset Manufacturer'].value_counts().reset_index()\n",
    "manufacturer_counts.columns = ['Handset Manufacturer', 'Count']\n",
    "\n",
    "# Sort the results\n",
    "sorted_manufacturers = manufacturer_counts.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Select the top 3 handset manufacturers\n",
    "top_3_manufacturers = sorted_manufacturers.head(3)\n",
    "\n",
    "# Print the top 3 handset manufacturers\n",
    "print(top_3_manufacturers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 handsets for A-Link Telecom International Co Limited:\n",
      "                      Handset Manufacturer                       Handset Type  \\\n",
      "0  A-Link Telecom International Co Limited   A-Link Telecom I. Cubot X18 Plus   \n",
      "1  A-Link Telecom International Co Limited         A-Link Telecom I. Cubot A5   \n",
      "2  A-Link Telecom International Co Limited  A-Link Telecom I. Cubot Note Plus   \n",
      "3  A-Link Telecom International Co Limited     A-Link Telecom I. Cubot Note S   \n",
      "4  A-Link Telecom International Co Limited       A-Link Telecom I. Cubot Nova   \n",
      "\n",
      "   Count  \n",
      "0      2  \n",
      "1      1  \n",
      "2      1  \n",
      "3      1  \n",
      "4      1  \n",
      "\n",
      "Top 5 handsets for ASUSTeK:\n",
      "  Handset Manufacturer                          Handset Type  Count\n",
      "5              ASUSTeK       Asustek Asus Zb552Kl Zenfone Go      7\n",
      "6              ASUSTeK   Asustek Asus Zenfone2 Laser Ze600Kl      5\n",
      "7              ASUSTeK   Asustek Asus Zenfone2 Laser Ze601Kl      4\n",
      "8              ASUSTeK  Asustek Asus Zenfone 2 Laser Ze500Kl      3\n",
      "9              ASUSTeK   Asustek Asus Zenfone Selfie Zd551Kl      2\n",
      "\n",
      "Top 5 handsets for Acer:\n",
      "   Handset Manufacturer           Handset Type  Count\n",
      "10                 Acer              Acer M310      5\n",
      "11                 Acer  Acer Liquid Zest Plus      2\n",
      "12                 Acer       Acer Liquid M220      1\n",
      "13                 Acer       Acer Liquid Z220      1\n",
      "14                 Acer       Acer Liquid Z530      1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify the top 5 handsets per top 3 handset manufacturer\n",
    "# Count handset manufacturers and types\n",
    "manufacturer_type_counts = raw_df.groupby(['Handset Manufacturer', 'Handset Type']).size().reset_index(name='Count')\n",
    "\n",
    "# Sort the results within each manufacturer\n",
    "sorted_manufacturer_types = manufacturer_type_counts.groupby('Handset Manufacturer').apply(lambda x: x.nlargest(5, 'Count')).reset_index(drop=True)\n",
    "\n",
    "# Print the top 5 handsets per top 3 handset manufacturers\n",
    "top_3_manufacturers = sorted_manufacturer_types['Handset Manufacturer'].unique()[:3]\n",
    "\n",
    "for manufacturer in top_3_manufacturers:\n",
    "    print(f\"Top 5 handsets for {manufacturer}:\")\n",
    "    manufacturer_handsets = sorted_manufacturer_types[sorted_manufacturer_types['Handset Manufacturer'] == manufacturer]\n",
    "    print(manufacturer_handsets)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying usersâ€™ behaviour on those Applications / Social Media, Google, Email, Youtube, Netflix, Gaming, Other.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the number of xDR sessions per user\n",
    "sessions_per_user = raw_df.groupby('MSISDN/Number')['Bearer Id'].count()\n",
    "print(sessions_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the session duration per user\n",
    "session_duration_per_user = raw_df.groupby('MSISDN/Number')['Dur. (ms)'].sum()\n",
    "\n",
    "# Display the result\n",
    "print(session_duration_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the total download (Dl) and upload data per user\n",
    "total_data_per_user = raw_df.groupby('MSISDN/Number')[['Total DL (Bytes)', 'Total UL (Bytes)']].sum()\n",
    "\n",
    "# Display the result\n",
    "print(total_data_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of application columns\n",
    "applications = ['Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']\n",
    "\n",
    "# Aggregate the total data volume per user and application\n",
    "total_data_per_user_app = raw_df.groupby('MSISDN/Number')[[col + ' DL (Bytes)' for col in applications] + [col + ' UL (Bytes)' for col in applications]].sum()\n",
    "\n",
    "# Display the result\n",
    "print(total_data_per_user_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanatory Data Analysis | EDA\n",
    "# Treat all missing values and outliers in the dataset by replacing by the mean of the corresponding column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Identifying and Treating missing value \n",
    "missing_values = raw_df.isna().sum()\n",
    "print (missing_values)\n",
    "\n",
    "for column in missing_values.index:\n",
    "    if missing_values[column] > 0:\n",
    "        mean_value = str(raw_df[column].mean())\n",
    "        raw_df[column] = raw_df[column].fillna(mean_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the basic metrics (mean, median, etc) from the dataset\n",
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct a Non-Graphical Univariate Analysis by computing dispersion parameters for each quantitative variable.\n",
    "\n",
    "quantitative_variables = []\n",
    "\n",
    "# Iterate over each column in the dataset to find the quantitative variable\n",
    "for column in raw_df.columns:   \n",
    "    if raw_df[column].dtype in [int, float]:\n",
    "        quantitative_variables.append(column)\n",
    "\n",
    "# Solution: By calculating the range or Difference b/n max and min value in each variable\n",
    "for column_name in quantitative_variables:\n",
    "    column_data = raw_df[column_name]\n",
    "    data_range = column_data.max() - column_data.min()\n",
    "    print(\"Range of\", column_name, \":\", data_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct a Graphical Univariate Analysis by identifying the most suitable plotting options for each variable and interpret your findings.\n",
    "# column_name = 'Avg RTT DL (ms)'\n",
    "clean_Data = raw_df.dropna()\n",
    "column_names = clean_Data.columns\n",
    "\n",
    "for column_name in column_names:\n",
    "    column_data = clean_Data[column_name]    \n",
    "    plt.hist(column_data, bins=10)\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of ' + column_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable transformations\n",
    "# Segment the users into top five decile classes based on the total duration for all sessions and compute the total data (DL+UL) per decile class. \n",
    "\n",
    "# Calculate the total duration for all sessions for each user\n",
    "user_total_duration = raw_df.groupby('MSISDN/Number')['Dur. (ms)'].sum()\n",
    "\n",
    "# Segment users into decile classes\n",
    "user_deciles = pd.qcut(user_total_duration, q=10, labels=False, duplicates='drop')\n",
    "\n",
    "# Compute the total data (DL+UL) per decile class\n",
    "data_per_decile = raw_df.groupby(user_deciles)[['Total DL (Bytes)', 'Total UL (Bytes)']].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis â€“ compute a correlation matrix for the following variables and interpret your findings: Social Media data, Google data, Email data, Youtube data, Netflix data, Gaming data, Other data \n",
    "# Select the columns for correlation analysis\n",
    "columns = [\n",
    "    'Social Media DL (Bytes)', 'Social Media UL (Bytes)',\n",
    "    'Google DL (Bytes)', 'Google UL (Bytes)',\n",
    "    'Email DL (Bytes)', 'Email UL (Bytes)',\n",
    "    'Youtube DL (Bytes)', 'Youtube UL (Bytes)',\n",
    "    'Netflix DL (Bytes)', 'Netflix UL (Bytes)',\n",
    "    'Gaming DL (Bytes)', 'Gaming UL (Bytes)',\n",
    "    'Other DL (Bytes)', 'Other UL (Bytes)'\n",
    "]\n",
    "\n",
    "# Create a subset dataframe with the selected columns\n",
    "subset_df = raw_df[columns]\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = subset_df.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  User Engagement analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the above metrics per customer id (MSISDN) and report the top 10 customers per engagement metric\n",
    "\n",
    "# Aggregate metrics per customer\n",
    "aggregated_data = raw_df.groupby('MSISDN/Number').agg({\n",
    "    'Bearer Id': 'nunique',  # Session frequency\n",
    "    'Dur. (ms)': 'sum',  # Session duration\n",
    "    'Total UL (Bytes)': 'sum',  # Upload traffic\n",
    "    'Total DL (Bytes)': 'sum'  # Download traffic\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns for better readability\n",
    "aggregated_data.rename(columns={\n",
    "    'Bearer Id': 'Session Frequency',\n",
    "    'Dur. (ms)': 'Session Duration',\n",
    "    'Total UL (Bytes)': 'Total Upload Traffic',\n",
    "    'Total DL (Bytes)': 'Total Download Traffic'\n",
    "}, inplace=True)\n",
    "\n",
    "# Report the top 10 customers per engagement metric\n",
    "top_10_frequency = aggregated_data.nlargest(10, 'Session Frequency')\n",
    "top_10_duration = aggregated_data.nlargest(10, 'Session Duration')\n",
    "top_10_upload_traffic = aggregated_data.nlargest(10, 'Total Upload Traffic')\n",
    "top_10_download_traffic = aggregated_data.nlargest(10, 'Total Download Traffic')\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 customers by Session Frequency:\")\n",
    "print(top_10_frequency)\n",
    "\n",
    "print(\"\\nTop 10 customers by Session Duration:\")\n",
    "print(top_10_duration)\n",
    "\n",
    "print(\"\\nTop 10 customers by Total Upload Traffic:\")\n",
    "print(top_10_upload_traffic)\n",
    "\n",
    "print(\"\\nTop 10 customers by Total Download Traffic:\")\n",
    "print(top_10_download_traffic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each engagement metric and run a k-means (k=3) to classify customers in three groups of engagement. \n",
    "# Aggregate metrics per customer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "aggregated_data = raw_df.groupby('MSISDN/Number').agg({\n",
    "    'Bearer Id': 'nunique',  # Session frequency\n",
    "    'Dur. (ms)': 'sum',  # Session duration\n",
    "    'Total UL (Bytes)': 'sum',  # Upload traffic\n",
    "    'Total DL (Bytes)': 'sum'  # Download traffic\n",
    "}).reset_index()\n",
    "\n",
    "# Normalize the engagement metrics\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(aggregated_data.iloc[:, 1:])  # Exclude customer ID\n",
    "\n",
    "# Run k-means clustering\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(normalized_data)\n",
    "\n",
    "# Add the cluster labels to the aggregated data\n",
    "aggregated_data['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Report the top customers per cluster\n",
    "top_customers_per_cluster = []\n",
    "for i in range(k):\n",
    "    cluster_customers = aggregated_data[aggregated_data['Cluster'] == i].nlargest(10, 'Bearer Id')\n",
    "    top_customers_per_cluster.append(cluster_customers)\n",
    "\n",
    "# Display the results\n",
    "for i, cluster_customers in enumerate(top_customers_per_cluster):\n",
    "    print(f\"\\nTop 10 customers in Cluster {i+1}:\")\n",
    "    print(cluster_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum, maximum, average & total non-normalized metrics for each cluster. \n",
    "# Interpret your results visually with accompanying text explaining your findings.\n",
    "\n",
    "# Aggregate metrics per customer\n",
    "aggregated_data = raw_df.groupby('MSISDN/Number').agg({\n",
    "    'Bearer Id': 'nunique',  # Session frequency\n",
    "    'Dur. (ms)': 'sum',  # Session duration\n",
    "    'Total UL (Bytes)': 'sum',  # Upload traffic\n",
    "    'Total DL (Bytes)': 'sum'  # Download traffic\n",
    "}).reset_index()\n",
    "\n",
    "# Normalize the engagement metrics\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(aggregated_data.iloc[:, 1:])  # Exclude customer ID\n",
    "\n",
    "# Run k-means clustering\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(normalized_data)\n",
    "\n",
    "# Add the cluster labels to the aggregated data\n",
    "aggregated_data['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Compute non-normalized metrics for each cluster\n",
    "cluster_metrics = aggregated_data.groupby('Cluster').agg({\n",
    "    'Bearer Id': ['min', 'max', 'mean', 'sum'],  # Session frequency\n",
    "    'Dur. (ms)': ['min', 'max', 'mean', 'sum'],  # Session duration\n",
    "    'Total UL (Bytes)': ['min', 'max', 'mean', 'sum'],  # Upload traffic\n",
    "    'Total DL (Bytes)': ['min', 'max', 'mean', 'sum']  # Download traffic\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "print(\"Non-normalized metrics for each cluster:\")\n",
    "print(cluster_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate user total traffic per application and derive the top 10 most engaged users per application\n",
    "\n",
    "# Extract the relevant columns for application traffic\n",
    "app_columns = ['MSISDN/Number', 'Social Media DL (Bytes)', 'Google DL (Bytes)', \n",
    "               'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', \n",
    "               'Gaming DL (Bytes)', 'Other DL (Bytes)']\n",
    "app_traffic = raw_df[app_columns].copy()\n",
    "\n",
    "# Rename the application columns for easier processing\n",
    "app_traffic.columns = ['MSISDN/Number', 'Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']\n",
    "\n",
    "# Melt the dataframe to combine all application columns into a single 'Application' column\n",
    "app_traffic = app_traffic.melt(id_vars='MSISDN/Number', var_name='Application', value_name='Total Traffic')\n",
    "\n",
    "# Aggregate total traffic per application for each user\n",
    "app_traffic = app_traffic.groupby(['MSISDN/Number', 'Application'])['Total Traffic'].sum().reset_index()\n",
    "\n",
    "# Derive the top 10 most engaged users per application\n",
    "top_users_per_app = []\n",
    "unique_apps = app_traffic['Application'].unique()\n",
    "\n",
    "for app in unique_apps:\n",
    "    top_users = app_traffic[app_traffic['Application'] == app].nlargest(10, 'Total Traffic')\n",
    "    top_users_per_app.append(top_users)\n",
    "\n",
    "# Display the results\n",
    "for i, app in enumerate(unique_apps):\n",
    "    print(f\"\\nTop 10 most engaged users for Application '{app}':\")\n",
    "    print(top_users_per_app[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 3 most used applications using appropriate charts. \n",
    "\n",
    "# Extract the relevant columns for application traffic\n",
    "app_columns = ['MSISDN/Number', 'Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', 'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)']\n",
    "app_traffic = raw_df[app_columns].copy()\n",
    "\n",
    "# Rename the application columns for easier processing\n",
    "app_traffic.columns = ['MSISDN/Number', 'Social Media', 'Google', 'Email', 'Youtube', 'Netflix', 'Gaming', 'Other']\n",
    "\n",
    "# Melt the dataframe to combine all application columns into a single 'Application' column\n",
    "app_traffic = app_traffic.melt(id_vars='MSISDN/Number', var_name='Application', value_name='Total Traffic')\n",
    "\n",
    "# Aggregate total traffic per application\n",
    "app_traffic = app_traffic.groupby('Application')['Total Traffic'].sum().reset_index()\n",
    "\n",
    "# Sort applications by total traffic in descending order\n",
    "app_traffic = app_traffic.sort_values('Total Traffic', ascending=False)\n",
    "\n",
    "# Select the top 3 most used applications\n",
    "top_3_apps = app_traffic.head(3)\n",
    "\n",
    "# Plot the top 3 most used applications\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(top_3_apps['Application'], top_3_apps['Total Traffic'])\n",
    "plt.xlabel('Application')\n",
    "plt.ylabel('Total Traffic')\n",
    "plt.title('Top 3 Most Used Applications')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using k-means clustering algorithm, group users in k engagement clusters based on the engagement metrics: \n",
    "# What is the optimized value of k (use elbow method for this)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
